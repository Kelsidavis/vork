# Qwen3 14B - Large Context Configuration
# Smaller model allows for much larger context window
# Expected VRAM: ~12-14GB total (fits comfortably on both GPUs)

default_backend = "llamacpp"

[assistant]
server_url = "http://localhost:8080"
model = "Qwen3-14B-UD-Q5_K_XL.gguf"
approval_policy = "never"
sandbox_mode = "danger-full-access"
require_git_repo = false

[llamacpp]
enabled = true
models_dir = "/media/k/vbox/models/Qwen3"
binary_path = "/home/k/llama.cpp/build/bin/llama-server"
context_size = 73728  # 72k context - maximum that fits in VRAM
ngl = 99  # All layers on GPU
threads = 20
batch_size = 512
parallel = 4
cache_type_k = "bf16"
cache_type_v = "bf16"

# Startup command:
# /home/k/llama.cpp/build/bin/llama-server \
#   -m /media/k/vbox/models/Qwen3/Qwen3-14B-UD-Q5_K_XL.gguf \
#   --host 0.0.0.0 --port 8080 \
#   -c 73728 -ngl 99 -t 20 -b 512 --parallel 4 \
#   --cache-type-k bf16 --cache-type-v bf16 --jinja

# Benefits:
# - 72k context (maximum that fits - entire large files, multiple files at once)
# - Faster inference than 30B model
# - Lower VRAM usage leaves room for larger batches
# - All layers fit on GPU with plenty of headroom

[ollama]
enabled = true
api_url = "http://localhost:11434"
