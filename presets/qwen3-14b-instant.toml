# Qwen3 14B - Instant Load Configuration
# Optimized for single GPU with maximum context that fits
# All layers on one GPU only - no secondary GPU needed
# Total VRAM: ~11GB (fits on RTX 3060 12GB or RTX 5080 16GB)

default_backend = "llamacpp"

[assistant]
server_url = "http://localhost:8080"
model = "Qwen3-14B-UD-Q5_K_XL.gguf"
approval_policy = "never"
sandbox_mode = "danger-full-access"
require_git_repo = false

[llamacpp]
enabled = true
models_dir = "/media/k/vbox/models/Qwen3"
binary_path = "/home/k/llama.cpp/build/bin/llama-server"
context_size = 8192  # Maximum that fits on single GPU
context_limit = 7168  # Compact when conversation exceeds this
ngl = 99  # All layers on GPU
threads = 20
batch_size = 512
parallel = 4
cache_type_k = "bf16"
cache_type_v = "bf16"
cuda_visible_devices = "1"  # Force to RTX 5080 (GPU 1)

# Startup command (force to RTX 5080 - GPU 1):
# CUDA_VISIBLE_DEVICES=1 /home/k/llama.cpp/build/bin/llama-server \
#   -m /media/k/vbox/models/Qwen3/Qwen3-14B-UD-Q5_K_XL.gguf \
#   --host 0.0.0.0 --port 8080 \
#   -c 8192 -ngl 99 -t 20 -b 512 --parallel 4 \
#   --cache-type-k bf16 --cache-type-v bf16 --jinja

# Benefits:
# - Fast startup (8k context fits comfortably)
# - Single GPU usage (~11GB total on RTX 3060/5080)
# - No multi-GPU complexity
# - Perfect for code reviews and medium-sized tasks
# - Fastest inference speed

[ollama]
enabled = true
api_url = "http://localhost:11434"
